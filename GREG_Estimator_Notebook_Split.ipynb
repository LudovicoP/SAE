{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e220025",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./Images/Illustration_general_methods.png\" alt=\"General Methods\" style=\"width: 60%; display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901fe44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<style>\n",
    ".reveal {\n",
    "    font-size: 10%;  /* Adjust this value to control the overall font size */\n",
    "}\n",
    "</style>\n",
    "# GREG Estimator\n",
    "\n",
    "$$\n",
    "\\hat{Y}_{i[\\text{GREG}]} = X_i'\\hat{\\beta} + \\sum_{k \\in s_i} d_{ik}(y_{ik} - x_{ik}'\\hat{\\beta})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\left( \\sum_{k \\in s_i} d_{ik}x_{ik}x_{ik}' \\right)^{-1} \\left( \\sum_{k \\in s_i} d_{ik}x_{ik}y_{ik}' \\right)\n",
    "$$\n",
    "\n",
    "Train a regression model on survey data ($Y$ & $X$), then replicate using SAE census data $X$.\n",
    "\n",
    "Clearly, you also have to account for $d$ (design weights) when training on survey data and when using the model. Then adjust to learn idiosyncratic part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6b9be",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GREG Estimator (Part 2)\n",
    "\n",
    "- Can be negative in small areas when it overestimates $Y$ (since $Y$ is a total, it must not be negative).\n",
    "- When there is no data, it is a model-based synthetic regression estimator (regression on $X$).\n",
    "- Approximately design unbiased.\n",
    "- Model unbiased only when area-specific auxiliary information is available under the assumption of linear association (relevance).\n",
    "- Not consistent (high residuals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda8717",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GREG Estimator (Revised Weights)\n",
    "\n",
    "A more general form can be derived if we take revised weights ($w_d \\times w_e$):\n",
    "\n",
    "- Consistency with covariate totals but not on the estimation of $Y$ at the aggregated level.\n",
    "- Provides a good estimate when covariates $X$ are consistent for the area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f534634d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modified Direct Estimator\n",
    "$$ \\hat{Y}_{i[\\text{MDE}]} = \\hat{Y}_i + (X_i - \\hat{X}_i)'\\hat{\\beta} \\qquad;\\qquad \\hat{\\beta} = \\left( \\sum_{k \\in s} d_k x_k x_k' \\right)^{-1} \\left( \\sum_{k \\in s} d_k x_k y_k \\right) $$\n",
    "\n",
    "- **Borrow strength** over small areas (SA) for estimating regression coefficients (using regression to remove systematic biases from the Horvitz-Thompson estimator).\n",
    "- **Improve estimator reliability**.\n",
    "- Approximate design unbiased as the overall sample size increases (although less effective compared to indirect small area estimators or design-based model-assisted estimators)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51530d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Modified Direct Estimator (Part 2)\n",
    "It's important to note:\n",
    "- $X_i$ is the true data from the census, and $\\hat{X}_i$ is the estimated data through HT or another basic estimator (similar to $\\hat{Y}_i$).\n",
    "- The adjustment is needed because, if $\\hat{X}_i$ is overestimated, $\\hat{Y}_i$ will likely be overestimated too. The regression corrects this overestimation.\n",
    "- Since $\\hat{Y}_i$ and the adjustment term are negatively correlated, this reduces the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1431db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias-Adj Synthetic Estimator\n",
    "\n",
    "## Synthetic Estimator:\n",
    "$$ \\hat{Y}_{i, \\text{syn}} = X_i' \\hat{\\beta} $$\n",
    "- **Assumption:** The relationship between $Y$ and $X$ is consistent across all areas.\n",
    "- **Issue:** If the true regression coefficients differ between areas, the synthetic estimator may have a large bias for area $i$.\n",
    "\n",
    "## Bias in Synthetic Estimator:\n",
    "For $x_{ij1} = 1$ (including an intercept),\n",
    "$$ \\mathbb{E}_D(\\hat{Y}_{i, \\text{syn}} - Y_i) \\neq 0 $$\n",
    "- **Explanation:** The bias arises because $\\hat{\\beta}$, estimated from all areas, may not reflect the true relationship in area $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37c801",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bias-Adj Synthetic Estimator (Part 2)\n",
    "## Bias Adjustment:\n",
    "To address this bias, an adjustment term is added:\n",
    "$$ \\hat{Y}_{i, \\text{adj}} = \\hat{Y}_{i, \\text{syn}} + (\\bar{y}_i - \\bar{x}_i' \\hat{\\beta}) $$\n",
    "- $\\bar{y}_i$: Sample mean of $Y$ in area $i$.\n",
    "- $\\bar{x}_i$: Sample mean of $X$ in area $i$.\n",
    "- The term $(\\bar{y}_i - \\bar{x}_i' \\hat{\\beta})$ estimates the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f77711",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bias-Adj Synthetic Estimator (Part 3)\n",
    "## Variance Impact:\n",
    "- **Variance Increases:** Incorporating $\\bar{y}_i$ increases the variance of the adjusted estimator:\n",
    "$$ \\text{Var}_D(\\hat{Y}_{i, \\text{adj}}) = O\\left(\\frac{1}{n_i}\\right) $$\n",
    "- Small areas with low sample sizes ($n_i$) will have larger variances.\n",
    "\n",
    "## Implication:\n",
    "- **Bias-Variance Trade-off:** \n",
    "    - Adjusting for bias reduces bias but increases variance.\n",
    "    - Small $n_i$ leads to high variance, making the estimate less reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1d59c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bias-Adj Synthetic Estimator (Part 4)\n",
    "## Summary:\n",
    "- The synthetic estimator may have large bias if area-specific relationships differ.\n",
    "- Adjusting for bias improves accuracy but can result in high variance, especially in small areas.\n",
    "- **Trade-off:** Reducing bias can increase variance, and alternative estimators may better balance the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ca12c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pros and Cons of Design-Based SAE\n",
    "\n",
    "<div style=\"height: 300px; overflow-y: scroll; font-size: 10px; padding: 10px; border: 1px solid #ccc;\">\n",
    "\n",
    "| **Advantages** | **Disadvantages** |\n",
    "| --- | --- |\n",
    "| **Model Independence:** Estimation is less dependent on an assumed model, although models assist in constructing the estimators. | **Large Variance in Direct Estimators:** Due to small sample sizes, direct estimators have high variance. |\n",
    "| **Unbiased and Consistent:** Estimators are approximately unbiased and consistent for large sample sizes within areas. This protects against possible model misspecification. | **Variable Survey Regression Estimators:** Survey regression estimators, though unbiased, may be too variable. |\n",
    "| **Protects Against Bias:** Especially effective in large areas, reducing the risk of bias from incorrect model assumptions. | **Bias in Synthetic Estimators:** While they have small variance, synthetic estimators are generally biased. |\n",
    "|  | **Complex Composite Estimators:** These have smaller bias but larger variance than synthetic estimators. Choosing the correct weights is challenging. |\n",
    "|  | **Conditional Inference Limitations:** Design-based methods do not easily lend themselves to conditional inference, which can inflate estimator variance. |\n",
    "|  | **Lack of Theory for No-Sample Areas:** There is no established theory for areas with no samples. Predictions in these areas are difficult without additional information. |\n",
    "|  | **Sample Size Issues:** Large sample normality assumptions may not hold in small areas, making confidence interval computations unreliable. |\n",
    "\n",
    "**Note:** Design-based methods are powerful but may not be effective for all small area estimations, especially where sample sizes are limited or nonexistent.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49023b87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Based Methods in SAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5554a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Explicit Model Approach\n",
    "\n",
    "**Basic Area**: You have area-specific survey data and census data, then use, e.g., the Fay and Herriot model to estimate $Y$ (aggregate information) you are interested in.\n",
    "\n",
    "- **Linking model**: $\\theta_i \\sim N(x_i'\\beta, \\sigma_e^2)$, where $i = 1, 2, \\ldots, n$; and $\\sigma_e$ is the variance of the model chosen\n",
    "- **Matching sampling model**: $\\hat{\\theta}_i | \\theta_i \\sim N(\\theta_i, \\omega_i^2)$, where $i = 1, 2, \\ldots, n$; and $\\omega_i$ is the known sampling variance which reflects the precision of the survey\n",
    "- Then you have a linear mixed model\n",
    "\n",
    "**Basic Unit**: You have individual-specific survey data and census data, then consider both area error and individual error (nested error linear regression).\n",
    "\n",
    "$$\n",
    "y_{ij} = x_{ij}'\\beta + \\epsilon_i + e_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99f6d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalized linear mixed model\n",
    "\n",
    "Key assumption that $\\epsilon$, $e$ are $N(0,\\sigma^2\\Phi)$ and uncorrelated.\n",
    "\n",
    "$$y = X\\beta + Z\\epsilon + e$$\n",
    "- **Example of GLMM (binomial logistic mixed model)**\n",
    "\n",
    "$$ LMM(y_{ik} | \\epsilon_i) = P(y_{ik} = 1 | \\epsilon_i) = \\frac{\\exp[x'_{ik}(\\beta + \\epsilon_i)]}{1 + \\exp[x'_{ik}(\\beta + \\epsilon_i)]}, \\quad k \\in E_i, \\forall i $$\n",
    "\n",
    "$$ \\hat{y}_{ik} = \\frac{\\exp[x'_{ik}(\\beta + \\epsilon_i)]}{1 + \\exp[x'_{ik}(\\beta + \\epsilon_i)]}, \\quad \\forall k \\in E_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cadf5a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methodologies in MMT\n",
    "\n",
    "MMT is essentially a way to simulate microdata in small areas to have reliable data to make estimates.\n",
    "\n",
    "- Create synthetic spatial microdata set (possible solution).\n",
    "- Synthetic reconstruction: construct synthetic micropopulation such that all small area-level constraints are reproduced.\n",
    "- Statistical data matching or fusion.\n",
    "- Iterative Proportional Fitting (IPF) with MCMC.\n",
    "- Reweighting: calibrates the sampling design weights to new based on a distance measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2937e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Data Matching or Fusion\n",
    "\n",
    "Data collected from two datasets matched using a unique variable (exact matching or uncertainty matching) $C = A \\cup B$.\n",
    "\n",
    "Empirical steps:\n",
    "\n",
    "1. Adjust available data and variable transformation.\n",
    "2. Choose matching variable.\n",
    "3. Select matching method and distance function.\n",
    "4. Validate.\n",
    "\n",
    "Matching is useful to have a more complete microdataset, and possibly $B$ is census data while $A$ is survey data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33a781",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IPF (Iterative Proportional Fitting)\n",
    "\n",
    "## Overview\n",
    "- **Purpose:** Adjust cell frequencies in contingency tables to match known marginal totals.\n",
    "- **Foundation:** Based on contingency table analysis and probability theory, the goal is to estimate $p(x, y)$.\n",
    "\n",
    "## Key Concepts\n",
    "- **Expected Marginal Totals:** The process uses known marginal totals and iteratively adjusts the cell values to ensure consistency.\n",
    "- **MCMC Sampling:** Used to reach a stationary distribution that reflects the true population structure.\n",
    "- **Fractional Weights:** The algorithm generates fractional weights for cells, which can be crucial for accurate representation of population estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f0e01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Important Considerations\n",
    "- **Number of Iterations:** Choose an appropriate number of iterations to ensure convergence without overfitting.\n",
    "- **Validation:** Both internal and external validation are needed to assess the accuracy and reliability of the generated data.\n",
    "- **Application Issues:** If the fractional weights are not integerized, they may not be suitable for agent-based models or other applications requiring discrete data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222eac1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IPF Process\n",
    "1. **Initialize Cell Estimates:**\n",
    "   - Start with initial estimates for each cell in the contingency table.\n",
    "   \n",
    "2. **Iterative Adjustment:**\n",
    "   - Adjust cell values iteratively by normalizing:\n",
    "     - **Over Columns:** \n",
    "     $$ p_{ij}^{(k+1)} = \\frac{p_{ij}^{(k)} \\times Q_i}{\\sum_j p_{ij}^{(k)}} $$\n",
    "     - **Over Rows:** \n",
    "     $$ p_{ij}^{(k+2)} = \\frac{p_{ij}^{(k+1)} \\times Q_j}{\\sum_i p_{ij}^{(k+1)}} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798e2f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. **Convergence Check:**\n",
    "   - The process stops when the sum of the updated cell values matches the predefined marginal totals:\n",
    "   $$ \\sum_j p_{ij}^{(m)} = Q_i \\quad \\text{and} \\quad \\sum_i p_{ij}^{(m)} = Q_j $$\n",
    "\n",
    "4. **Validation:**\n",
    "   - Validate the final cell values against the known marginal totals and ensure the output aligns with the expected distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a78c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IPF Application\n",
    "- **Synthetic Data Creation:** Used to create synthetic spatial microdata for analysis and modeling.\n",
    "- **Microsimulation Modeling:** Provides fractional weights crucial for microsimulation modeling and policy evaluations.\n",
    "\n",
    "**Note:** While IPF is robust for adjusting cell frequencies to known totals, issues arise when these weights need to be used in models that require discrete values. Methodological advancements are ongoing to address these limitations and improve its application scope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283063b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Repeated Weighting Method (RWM)\n",
    "\n",
    "## Overview\n",
    "- **Purpose:** Estimate well-defined table sets from a large database, ensuring numerical consistency across estimates.\n",
    "- **Context:** May encounter inconsistencies due to the Social Statistical Database (SSD) based on different data sources.\n",
    "\n",
    "## Key Features\n",
    "- **GREG Estimator:** Based on repeated use of the Generalized Regression (GREG) estimator due to its strong calibration properties.\n",
    "- **Consistency:** The method ensures that all estimated tables are numerically consistent with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a7003",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Steps in the Repeated Weighting Process\n",
    "1. **Estimate Tables from SSD:**\n",
    "   - Begin by estimating tables from a large, comprehensive database (SSD), using all available data sources.\n",
    "\n",
    "2. **Identify Common Margins:**\n",
    "   - For each table, determine which margins are in common with previously estimated tables in the set.\n",
    "\n",
    "3. **Calibrate on Common Margins:**\n",
    "   - Estimate the table by calibrating on these common margins, ensuring numerical consistency across the entire set of tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff655b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Benefits\n",
    "- **Reduced Variance:** This method helps to lower variance in estimates by utilizing common margins effectively.\n",
    "- **Maximized Information Use:** Makes optimal use of all available data and auxiliary information.\n",
    "\n",
    "## Limitations\n",
    "- **Not Universally Applicable:** May not be suitable for all situations, particularly when rapid estimates or estimates for multiple subpopulations are required.\n",
    "- **Inconsistencies with SSD:** Estimates may vary due to differences in data sources, potentially leading to inconsistencies in large-scale statistical dissemination.\n",
    "\n",
    "**Note:** This method is most effective when numerical consistency is crucial and when working with well-defined, structured datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1655db9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CO (Combination of Individuals)\n",
    "\n",
    "- Fit an appropriate combination of individuals given known benchmarks.\n",
    "- Iterative process: select randomly and replace an individual trying to improve the SA benchmark.\n",
    "- Techniques: hill climbing, simulated annealing, genetic algorithms (otherwise combinatorial).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcae405",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GREGWT Approach\n",
    "\n",
    "## Overview\n",
    "- **What is GREGWT?**\n",
    "  - GREGWT (Generalized Regression Weighting Tool) is an extension of the GREG estimator.\n",
    "  - Unlike GREG, it uses aggregate information for calibration instead of individual-level data ($X$).\n",
    "\n",
    "## Key Features\n",
    "- **Iterative Calibration:**\n",
    "  - Uses an iterative GREG algorithm, often implemented in SAS, to adjust survey weights.\n",
    "  - Calibrates survey estimates to match known benchmarks, ensuring consistency with external totals.\n",
    "\n",
    "- **Auxiliary Information:**\n",
    "  - Incorporates external sources of data (auxiliary information) to improve the accuracy and reliability of survey estimates.\n",
    "\n",
    "- **Constrained Distance Function:**\n",
    "  - Minimizes a truncated chi-squared distance function, subject to calibration equations for each small area.\n",
    "  - Known as \"truncated linear regression\" or \"restricted modified chi-squared\" method.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2b7e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Steps in the GREGWT Process\n",
    "1. **Define Calibration Equation:**\n",
    "   - Set up a calibration equation for each small area:\n",
    "   $$ \\sum_{k \\in s} w_k x_k = T_x $$\n",
    "\n",
    "2. **Adjust Weights:**\n",
    "   - Weights are adjusted using Lagrange multipliers:\n",
    "   $$ w_k = d_k + d_k x_k' \\lambda $$\n",
    "   where $\\lambda$ is the vector of Lagrange multipliers calculated iteratively.\n",
    "\n",
    "3. **Minimize Distance Function:**\n",
    "   - Minimize the truncated chi-squared distance function using the Newton-Raphson method:\n",
    "   $$ \\lambda = \\left( \\sum_{k \\in s} d_k x_k x_k' \\right)^{-1} (T_x - t_{x,s}) $$\n",
    "\n",
    "4. **Iterate Until Convergence:**\n",
    "   - Repeat the process until the weights converge to satisfy the calibration equation for each small area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc3b18",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications\n",
    "- **Survey Estimation:**\n",
    "  - Used for adjusting survey weights to match known benchmarks, improving the representativeness of survey data.\n",
    "\n",
    "- **Small Area Estimation:**\n",
    "  - Particularly useful for small area estimation where individual-level data is unavailable, but aggregate benchmarks are known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8262c",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "- **Boundary Conditions:**\n",
    "  - Weights must lie within predefined boundary conditions, which can be restrictive.\n",
    "  \n",
    "- **Complex Implementation:**\n",
    "  - Requires complex iterative algorithms and may not converge in certain scenarios without adjustments.\n",
    "\n",
    "**Summary:**\n",
    "- GREGWT is a powerful tool for calibrating survey estimates using auxiliary information. It ensures consistency with benchmarks through an iterative process that minimizes a constrained distance function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592704c7",
   "metadata": {},
   "source": [
    "# GREGWT vs. CO Reweighting Methodologies\n",
    "<div style=\"height: 300px; overflow-y: scroll; font-size: 10px; padding: 10px; border: 1px solid #ccc;\">\n",
    "\n",
    "| **Aspect** | **GREGWT** | **CO** |\n",
    "| --- | --- | --- |\n",
    "| **Process Type** | Iterative process using the Newton-Raphson method. | Iterative process with a stochastic approach. |\n",
    "| **Basis** | Minimizes a distance function based on known benchmarks. | Based on a combination of households to best fit known benchmarks. |\n",
    "| **Minimization Tools** | Uses Lagrange multipliers for minimizing the distance function. | Uses CO techniques as intelligent searching tools for optimizing household combinations. |\n",
    "| **Weights** | Fractional weights. | Integer weights. |\n",
    "| **Boundary Conditions** | Applies new weights for boundary conditions. | No boundary conditions applied. |\n",
    "| **Benchmark Constraints** | Fixed for the algorithm; sensitive to disagreements between benchmarks. | Flexible; insensitive to benchmark disagreements and can split the difference between constraints. |\n",
    "| **Focus** | Simulating microdata at small area levels; aggregation possible at larger domains. | Allows for mutually consistent analysis at any aggregation or sophistication level. |\n",
    "| **Standard Errors** | Obtained via a group jackknife approach. | No known method for standard error estimation. |\n",
    "| **Convergence** | May not exist in some cases; requires boundary adjustments. | No convergence issues; however, final household combination may still fail to fit benchmark constraints. |\n",
    "| **Statistical Reliability** | No standard index for checking statistical reliability. | No standard index for statistical reliability. |\n",
    "| **Stability** | Iteration may be unstable near a horizontal asymptote or local extremum. | Iteration may avoid local extrema, but can be deceiving at local solutions. |\n",
    "</div>\n",
    "\n",
    "**Summary:**\n",
    "- **GREGWT** is highly sensitive to benchmarks and can be unstable in some cases, but is effective for simulating microdata at small area levels.\n",
    "- **CO** offers greater flexibility with integer weights and is less sensitive to benchmark disagreements, but lacks established methods for standard errors and statistical reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6c811",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Estimate Standard Error (JACKKNIFE Approach)\n",
    "\n",
    "The SA estimates have their own standard errors, and GREGWT can calculate these using a group jackknife approach:\n",
    "\n",
    "1. Divide the survey into subsamples and calculate jackknife estimates for each based on the total sample excluding the subsample in use (leave-one-out).\n",
    "2. Take the difference between the new estimate and the original estimate and use it to estimate variance $\\rightarrow$ standard error.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
